{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import scipy.misc\n",
    "import numpy as np\n",
    "from utils import *\n",
    "from VESPCN_utils import *\n",
    "import tensorflow as tf\n",
    "from easydict import EasyDict as edict\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "config = edict()\n",
    "\n",
    "config.sample_dir = \"samples_MCT\"\n",
    "config.checkpoint_dir = \"checkpoint/MCT\"\n",
    "config.log_dir = \"logs\"\n",
    "config.train_size = 100000000 # use large number if you have enough memory\n",
    "config.valid_size = 10 # use large number if you have enough memory\n",
    "config.test_size = 6400 # use large number if you have enough memory\n",
    "config.batch_size = 8 # use large number if you have enough memory\n",
    "config.patch_shape = [50,50,3] #[51,51,3]\n",
    "config.scale = 3 #3\n",
    "config.learning_rate = 1e-5\n",
    "config.epoch = 1000000\n",
    "config.channels = 3\n",
    "config.mode = \"RGB\"\n",
    "\n",
    "#config.channels = 1\n",
    "#config.mode = \"YCbCr\"\n",
    "\n",
    "#'''\n",
    "config.dataset = \"CDVL\"\n",
    "config.num_videos = 76\n",
    "\n",
    "#'''\n",
    "config.train = edict()\n",
    "config.train.lr_init = 1e-3\n",
    "config.train.lr_decay = 0.5\n",
    "config.train.decay_iter = 10\n",
    "config.augmentation = True\n",
    "\n",
    "if not os.path.exists(config.checkpoint_dir):\n",
    "    os.makedirs(config.checkpoint_dir)\n",
    "if not os.path.exists(config.sample_dir):\n",
    "    os.makedirs(config.sample_dir)\n",
    "if not os.path.exists(config.log_dir):\n",
    "    os.makedirs(config.log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import os\n",
    "import time\n",
    "from glob import glob\n",
    "import tensorflow as tf\n",
    "import scipy.misc\n",
    "from subpixel import PS\n",
    "import numpy as np\n",
    "\n",
    "from utils import *\n",
    "from VESPCN_utils import *\n",
    "from warp import *\n",
    "\n",
    "class MotionCompensationTransformer(object):\n",
    "    def __init__(self, sess, config):\n",
    "        self.sess = sess\n",
    "        self.config = config\n",
    "        self.batch_size = config.batch_size\n",
    "        self.valid_size = config.batch_size\n",
    "        self.patch_shape = config.patch_shape\n",
    "        self.input_size = int(config.patch_shape[0])\n",
    "        self.dataset_name = config.dataset\n",
    "        self.mode = config.mode\n",
    "        self.channels = config.channels\n",
    "        self.augmentation = config.augmentation\n",
    "        self.checkpoint_dir = config.checkpoint_dir\n",
    "        self.build_model()\n",
    "        tf.global_variables_initializer().run(session=self.sess)\n",
    "        #x = self.sess.run([self.output], feed_dict = {self.input_t0: np.zeros([8,48,48,self.channels]), self.input_t1: np.zeros([8,48,48,self.channels])})\n",
    "        #print(x[0].shape)\n",
    "        self.num_videos = config.num_videos\n",
    "\n",
    "    def build_model(self):   \n",
    "        \n",
    "        identity_x = np.zeros([self.batch_size, self.input_size, self.input_size, 1])\n",
    "        identity_y = np.zeros([self.batch_size, self.input_size, self.input_size, 1])\n",
    "        for i in range(0, self.batch_size):\n",
    "            for j in range(0, self.input_size):\n",
    "                for k in range(0, self.input_size):\n",
    "                    identity_x[i,j,k] = 2.0*j/(self.input_size-1) -1\n",
    "                    identity_y[i,j,k] = 2.0*k/(self.input_size-1) -1\n",
    "        self.id_x = tf.constant(identity_x, dtype = tf.float32)  \n",
    "        self.id_y = tf.constant(identity_y, dtype = tf.float32)   \n",
    "        \n",
    "        #input frames\n",
    "        self.input_t0 = tf.placeholder(tf.float32, [self.batch_size, self.input_size, self.input_size, self.channels], name='input_t0') \n",
    "        self.input_t1 = tf.placeholder(tf.float32, [self.batch_size, self.input_size, self.input_size, self.channels], name='input_t1') \n",
    "\n",
    "        #output frame (compensated t1)\n",
    "        self.output,self.coarse_x, self.coarse_y = self.network(self.input_t0, self.input_t1)\n",
    "        print(\"output shape:\", self.output.shape)\n",
    "        \n",
    "        #for unknown sizes\n",
    "        self.input2_t0 = tf.placeholder(tf.float32, [1, 480, 720, self.channels], name='input_t0_unkown')\n",
    "        self.input2_t1 = tf.placeholder(tf.float32, [1, 480, 720, self.channels], name='input_t1_unkown')\n",
    "        self.output2,self.coarse_x2, self.coarse_y2 = self.network(self.input2_t0, self.input2_t1)\n",
    "\n",
    "        self.loss = tf.reduce_mean(tf.square(self.input_t0-self.output)) \\\n",
    "        #+ 0.01 * tf.reduce_mean(tf.sqrt(tf.add(tf.square(self.coarse_x)+tf.square(self.coarse_y),1e-4)))\n",
    "        + 0.01 * tf.reduce_mean(tf.sqrt(tf.add(tf.square(self.coarse_x-self.id_x)+tf.square(self.coarse_y-self.id_y),1e-4))) #approximated Huber loss\n",
    "        self.vars = tf.trainable_variables()\n",
    "        print(\"Number of variables in network:\",len(self.vars),\", full list:\",self.vars)\n",
    "        self.optimizer = tf.train.AdamOptimizer(self.config.learning_rate).minimize(self.loss, var_list=self.vars)\n",
    "\n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "    def network(self, t0, t1):\n",
    "        '''\n",
    "        ######## coasrse flow ######\n",
    "        tmp = tf.concat([t0, t1], axis = 3) #early fusion\n",
    "        tmp = tf.layers.conv2d(tmp, 24, 5, strides = 2, padding = 'SAME', name = 'Coarse_1', \n",
    "                               kernel_initializer = tf.contrib.layers.xavier_initializer(), reuse=tf.AUTO_REUSE)\n",
    "        tmp = tf.nn.relu(tmp)\n",
    "        tmp = tf.layers.conv2d(tmp, 24, 3, strides = 1, padding = 'SAME', name = 'Coarse_2', \n",
    "                               kernel_initializer = tf.contrib.layers.xavier_initializer(), reuse=tf.AUTO_REUSE)\n",
    "        tmp = tf.nn.relu(tmp)\n",
    "        tmp = tf.layers.conv2d(tmp, 24, 5, strides = 2, padding = 'SAME', name = 'Coarse_3', \n",
    "                               kernel_initializer = tf.contrib.layers.xavier_initializer(), reuse=tf.AUTO_REUSE)\n",
    "        tmp = tf.nn.relu(tmp)\n",
    "        tmp = tf.layers.conv2d(tmp, 24, 3, strides = 1, padding = 'SAME', name = 'Coarse_4', \n",
    "                               kernel_initializer = tf.contrib.layers.xavier_initializer(), reuse=tf.AUTO_REUSE)\n",
    "        tmp = tf.nn.relu(tmp)\n",
    "        tmp = tf.layers.conv2d(tmp, 32, 3, strides = 1, padding = 'SAME', name = 'Coarse_5', \n",
    "                               kernel_initializer = tf.contrib.layers.xavier_initializer(), reuse=tf.AUTO_REUSE)\n",
    "        tmp = tf.nn.tanh(tmp)\n",
    "\n",
    "        coarse_x = tf.layers.conv2d(tmp, 4*4*1, 3, strides = 1, padding = 'SAME', name = 'Coarse_x', \n",
    "                               kernel_initializer = tf.contrib.layers.xavier_initializer(), reuse=tf.AUTO_REUSE)\n",
    "        coarse_y = tf.layers.conv2d(tmp, 4*4*1, 3, strides = 1, padding = 'SAME', name = 'Coarse_y', \n",
    "                               kernel_initializer = tf.contrib.layers.xavier_initializer(), reuse=tf.AUTO_REUSE)\n",
    "        coarse_x = PS(coarse_x, 4, color=False)\n",
    "        coarse_y = PS(coarse_y, 4, color=False)\n",
    "        #print(\"shape: \", coarse_x.shape, coarse_y.shape)\n",
    "        \n",
    "        ######## fine flow ######\n",
    "        tmp = tf.concat([t0, t1], axis = 3) #early fusion\n",
    "        tmp = tf.layers.conv2d(tmp, 24, 5, strides = 2, padding = 'SAME', name = 'Fine_1', \n",
    "                               kernel_initializer = tf.contrib.layers.xavier_initializer(), reuse=tf.AUTO_REUSE)\n",
    "        tmp = tf.nn.relu(tmp)\n",
    "        tmp = tf.layers.conv2d(tmp, 24, 3, strides = 1, padding = 'SAME', name = 'Fine_2', \n",
    "                               kernel_initializer = tf.contrib.layers.xavier_initializer(), reuse=tf.AUTO_REUSE)\n",
    "        tmp = tf.nn.relu(tmp)\n",
    "        tmp = tf.layers.conv2d(tmp, 24, 3, strides = 1, padding = 'SAME', name = 'Fine_3', \n",
    "                               kernel_initializer = tf.contrib.layers.xavier_initializer(), reuse=tf.AUTO_REUSE)\n",
    "        tmp = tf.nn.relu(tmp)\n",
    "        tmp = tf.layers.conv2d(tmp, 24, 3, strides = 1, padding = 'SAME', name = 'Fine_4', \n",
    "                               kernel_initializer = tf.contrib.layers.xavier_initializer(), reuse=tf.AUTO_REUSE)\n",
    "        tmp = tf.nn.relu(tmp)\n",
    "        tmp = tf.layers.conv2d(tmp, 8, 3, strides = 1, padding = 'SAME', name = 'Fine_5', \n",
    "                               kernel_initializer = tf.contrib.layers.xavier_initializer(), reuse=tf.AUTO_REUSE)\n",
    "        tmp = tf.nn.tanh(tmp)\n",
    "\n",
    "        fine_x = tf.layers.conv2d(tmp, 2*2*1, 3, strides = 1, padding = 'SAME', name = 'Fine_x', \n",
    "                               kernel_initializer = tf.contrib.layers.xavier_initializer(), reuse=tf.AUTO_REUSE)\n",
    "        fine_y = tf.layers.conv2d(tmp, 2*2*1, 3, strides = 1, padding = 'SAME', name = 'Fine_y', \n",
    "                               kernel_initializer = tf.contrib.layers.xavier_initializer(), reuse=tf.AUTO_REUSE)\n",
    "        fine_x = PS(fine_x, 2, color=False)\n",
    "        fine_y = PS(fine_y, 2, color=False)\n",
    "        \n",
    "        #add coarse, fine flow\n",
    "        flow_x = tf.add(coarse_x, fine_x)\n",
    "        flow_y = tf.add(coarse_y, fine_y)\n",
    "        flow = tf.concat([flow_x, flow_y], axis = 3)\n",
    "        #Warp\n",
    "        out = batch_warp2d(t1, flow)\n",
    "        #print(\"shape:\", t1.shape, flow.shape, out.shape)\n",
    "        return out\n",
    "        '''\n",
    "         ######## coasrse flow ######\n",
    "        tmp = tf.concat([t0, t1], axis = 3) #early fusion\n",
    "        tmp = tf.layers.conv2d(tmp, 24, 5, strides = 1, padding = 'SAME', name = 'Coarse_1', \n",
    "                               kernel_initializer = tf.contrib.layers.xavier_initializer(), reuse=tf.AUTO_REUSE)\n",
    "        tmp = tf.nn.relu(tmp)\n",
    "        tmp = tf.layers.conv2d(tmp, 24, 3, strides = 1, padding = 'SAME', name = 'Coarse_2', \n",
    "                               kernel_initializer = tf.contrib.layers.xavier_initializer(), reuse=tf.AUTO_REUSE)\n",
    "        tmp = tf.nn.relu(tmp)\n",
    "        tmp = tf.layers.conv2d(tmp, 24, 5, strides = 1, padding = 'SAME', name = 'Coarse_3', \n",
    "                               kernel_initializer = tf.contrib.layers.xavier_initializer(), reuse=tf.AUTO_REUSE)\n",
    "        tmp = tf.nn.relu(tmp)\n",
    "        tmp = tf.layers.conv2d(tmp, 24, 3, strides = 1, padding = 'SAME', name = 'Coarse_4', \n",
    "                               kernel_initializer = tf.contrib.layers.xavier_initializer(), reuse=tf.AUTO_REUSE)\n",
    "        tmp = tf.nn.relu(tmp)\n",
    "        tmp = tf.layers.conv2d(tmp, 32, 3, strides = 1, padding = 'SAME', name = 'Coarse_5', \n",
    "                               kernel_initializer = tf.contrib.layers.xavier_initializer(), reuse=tf.AUTO_REUSE)\n",
    "        #tmp = tf.nn.tanh(tmp)\n",
    "        tmp = tf.nn.relu(tmp)\n",
    "\n",
    "        coarse_x = tf.layers.conv2d(tmp, 1, 1, strides = 1, padding = 'SAME', name = 'Coarse_x', \n",
    "                               kernel_initializer = tf.contrib.layers.xavier_initializer(), reuse=tf.AUTO_REUSE)\n",
    "        coarse_y = tf.layers.conv2d(tmp, 1, 1, strides = 1, padding = 'SAME', name = 'Coarse_y', \n",
    "                               kernel_initializer = tf.contrib.layers.xavier_initializer(), reuse=tf.AUTO_REUSE)\n",
    "\n",
    "        #coarse_x = PS(coarse_x, 4, color=False)\n",
    "        #coarse_y = PS(coarse_y, 4, color=False)\n",
    "        n, h, w, _ = t0.get_shape().as_list()\n",
    "        base_x = np.zeros([n, h, w, 1])\n",
    "        base_y = np.zeros([n, h, w, 1])\n",
    "        for i in range(0, n):\n",
    "            for j in range(0, h):\n",
    "                for k in range(0, w):\n",
    "                    base_x[i,j,k] = 2.0*j/(h-1) -1\n",
    "                    base_y[i,j,k] = 2.0*k/(w-1) -1\n",
    "        _x = tf.constant(base_x, dtype = tf.float32)  \n",
    "        _y = tf.constant(base_y, dtype = tf.float32)\n",
    "        #Warp\n",
    "        #flow_x = tf.add(tf.multiply(coarse_x,1e-10), _x)\n",
    "        #flow_y = tf.add(tf.multiply(coarse_y,1e-10), _y)\n",
    "        flow_x = tf.add(coarse_x, _x)\n",
    "        flow_y = tf.add(coarse_y, _y)\n",
    "        out = batch_warp2d_2(t1, flow_x, flow_y)\n",
    "\n",
    "        return out, coarse_x, coarse_y\n",
    "                \n",
    "    def train(self, config, load = True):\n",
    "        # setup train/validation data\n",
    "        '''\n",
    "        valid = sorted(glob(os.path.join(self.config.valid.hr_path, \"*.png\")))\n",
    "        shuffle(valid)\n",
    "        \n",
    "        valid_files = valid[0:self.valid_size]\n",
    "        valid = [load_image(valid_file, self.mode) for valid_file in valid_files]\n",
    "        valid_LR = [doresize(xx, [self.input_size,]*2) for xx in valid]\n",
    "        valid_HR = np.array(valid)\n",
    "        valid_LR = np.array(valid_LR)\n",
    "        if self.mode == \"YCbCr\":\n",
    "            valid_RGB_HR =  np.copy(valid_HR)\n",
    "            valid_HR = np.split(valid_RGB_HR,3, axis=3)[0]\n",
    "            valid_RGB_LR = np.copy(valid_LR)\n",
    "            valid_LR = np.split(valid_RGB_LR,3, axis=3)[0]\n",
    "        '''\n",
    "        counter = 1\n",
    "        start_time = time.time()\n",
    "        if load == True:\n",
    "            if self.load(self.checkpoint_dir):\n",
    "                print(\" [*] Load SUCCESS\")\n",
    "            else:\n",
    "                print(\" [!] Load failed...\")\n",
    "        else:\n",
    "            print(\" Training starts from beginning\")\n",
    "\n",
    "        for epoch in range(self.config.epoch):\n",
    "            if epoch % 100 == 0:\n",
    "                print(\"Loading videos again...\")\n",
    "                self.imdb = []\n",
    "                self.num_frames_per_video = []\n",
    "                self.imdb, self.num_frames_per_video = load_videos(30, self.num_videos, 50, self.mode)\n",
    "            batch_idxs = min(len(self.imdb), self.config.train_size) // self.config.batch_size\n",
    "\n",
    "            #for idx in range(0, batch_idxs):\n",
    "            for idx in range(0, 100):\n",
    "                batch_t0, batch_t1 = get_batch_MCT(self.imdb, self.num_frames_per_video, self.batch_size, \n",
    "                                                   [self.input_size, self.input_size], augmentation = self.augmentation)\n",
    "                batch_t0 = np.array(batch_t0)\n",
    "                batch_t1 = np.array(batch_t1)\n",
    "\n",
    "                _, loss = self.sess.run([self.optimizer, self.loss],\n",
    "                    feed_dict={ self.input_t0: batch_t0, self.input_t1: batch_t1 })\n",
    "\n",
    "                counter+=1\n",
    "                if idx % 500 == 1 and epoch % 100 == 0:\n",
    "                    #print(\"Epoch: [%2d] [%4d/%4d] time: %4.4f, loss: %.8f\" %(epoch, idx, batch_idxs, time.time() - start_time, loss))\n",
    "                    print(\"Epoch: [%2d] [%4d/%4d] time: %4.4f, loss: %.8f\" %(epoch, idx, 100, time.time() - start_time, loss))\n",
    "                    self.save(self.config.checkpoint_dir)\n",
    "             \n",
    "            # occasional testing\n",
    "            if epoch % 100 == 0:\n",
    "                avg_PSNR_original, avg_PSNR_MCT = self.test(load = False, epoch = epoch)\n",
    "                print(\"Epoch: [%2d] test PSNR original, MTC: %.6f, %.6f\" % (epoch, avg_PSNR_original, avg_PSNR_MCT))\n",
    "        self.save(self.config.checkpoint_dir)\n",
    "    \n",
    "    def test(self, name = \"foliage\", epoch = 0, load = True):\n",
    "        result_dir = os.path.join(\"./samples_MCT/\",str(name))\n",
    "        if not os.path.exists(result_dir):\n",
    "            os.makedirs(result_dir)\n",
    "        img_list = sorted(glob(os.path.join(\"/home/johnyi/deeplearning/research/VSR_Datasets/test/\",str(name),\"*.png\")))\n",
    "        \n",
    "        if load == True:\n",
    "            if self.load(self.checkpoint_dir):\n",
    "                print(\" [*] Load SUCCESS\")\n",
    "            else:\n",
    "                print(\" [!] Load failed...\")\n",
    "        avg_PSNR_original = 0\n",
    "        avg_PSNR_MCT = 0\n",
    "        xx = sorted(glob(os.path.join(\"/home/johnyi/deeplearning/research/VSR_Datasets/test/vid4\", \"*.png\")))\n",
    "        frame_t0 = scipy.misc.imread(\"/home/johnyi/deeplearning/research/VSR_Datasets/test/vid4/\"+str(name)+\"/001.png\", mode = self.mode)\n",
    "        frame_t1 = scipy.misc.imread(\"/home/johnyi/deeplearning/research/VSR_Datasets/test/vid4/\"+str(name)+\"/002.png\", mode = self.mode)\n",
    "\n",
    "        out, flow_x, flow_y = self.sess.run([self.output2, self.coarse_x2, self.coarse_y2],\n",
    "                    feed_dict={ self.input2_t0: [frame_t0], self.input2_t1:[frame_t1] })\n",
    "        #print(\"out:\",np.squeeze(out[0]).shape)\n",
    "        #print(\"flow:\", flow_x, flow_y)\n",
    "        #print(np.squeeze(out[0]))\n",
    "        PSNR_original = calc_PSNR(frame_t0, frame_t1)\n",
    "        #PSNR_MCT = calc_PSNR(frame_t0, out[0][0,:,:,:])\n",
    "        PSNR_MCT = calc_PSNR(frame_t0, out[0,:,:,:])\n",
    "        avg_PSNR_original += PSNR_original\n",
    "        avg_PSNR_MCT += PSNR_MCT\n",
    "        \n",
    "        imageio.imwrite(result_dir+\"/original_0.png\", frame_t0)\n",
    "        imageio.imwrite(result_dir+\"/original_1.png\", frame_t1)\n",
    "        #imageio.imwrite(result_dir+\"/compensated_0_\"+str(epoch)+\".png\", np.squeeze(out[0]))\n",
    "        imageio.imwrite(result_dir+\"/compensated_0_\"+str(epoch)+\".png\", out[0])\n",
    "        print(\"PSNR original:\", PSNR_original, \"MCT: \", PSNR_MCT)\n",
    "        return avg_PSNR_original, avg_PSNR_MCT\n",
    "    \n",
    "    def save(self, checkpoint_dir):\n",
    "        model_name = \"MCT-\"+str(self.mode)\n",
    "        model_dir = \"%s\" % (self.dataset_name)\n",
    "        checkpoint_dir = os.path.join(checkpoint_dir, model_dir)\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        self.saver.save(self.sess, os.path.join(checkpoint_dir, model_name))\n",
    "\n",
    "    def load(self, checkpoint_dir):\n",
    "        print(\" [*] Reading checkpoints...\")\n",
    "        model_dir = \"%s\"% (self.dataset_name)\n",
    "        checkpoint_dir = os.path.join(checkpoint_dir, model_dir)\n",
    "        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
    "        print(\"loading from \",checkpoint_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n",
    "            model_name = \"MCT-\"+str(self.mode)\n",
    "            self.saver.restore(self.sess, os.path.join(checkpoint_dir, model_name))\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape: (?, ?, ?, ?)\n",
      "Number of variables in network: 14 , full list: [<tf.Variable 'Coarse_1/kernel:0' shape=(5, 5, 6, 24) dtype=float32_ref>, <tf.Variable 'Coarse_1/bias:0' shape=(24,) dtype=float32_ref>, <tf.Variable 'Coarse_2/kernel:0' shape=(3, 3, 24, 24) dtype=float32_ref>, <tf.Variable 'Coarse_2/bias:0' shape=(24,) dtype=float32_ref>, <tf.Variable 'Coarse_3/kernel:0' shape=(5, 5, 24, 24) dtype=float32_ref>, <tf.Variable 'Coarse_3/bias:0' shape=(24,) dtype=float32_ref>, <tf.Variable 'Coarse_4/kernel:0' shape=(3, 3, 24, 24) dtype=float32_ref>, <tf.Variable 'Coarse_4/bias:0' shape=(24,) dtype=float32_ref>, <tf.Variable 'Coarse_5/kernel:0' shape=(3, 3, 24, 32) dtype=float32_ref>, <tf.Variable 'Coarse_5/bias:0' shape=(32,) dtype=float32_ref>, <tf.Variable 'Coarse_x/kernel:0' shape=(1, 1, 32, 1) dtype=float32_ref>, <tf.Variable 'Coarse_x/bias:0' shape=(1,) dtype=float32_ref>, <tf.Variable 'Coarse_y/kernel:0' shape=(1, 1, 32, 1) dtype=float32_ref>, <tf.Variable 'Coarse_y/bias:0' shape=(1,) dtype=float32_ref>]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "MCT = MotionCompensationTransformer(sess, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training starts from beginning\n",
      "Loading videos again...\n",
      "loaded video indexes: [52, 70, 14, 28, 65, 46, 31, 43, 39, 8, 61, 34, 48, 63, 20, 7, 36, 71, 3, 2, 16, 41, 42, 1, 10, 73, 37, 74, 54, 15] num_frames per video: 50 runtime:  72.0022828578949\n",
      "Epoch: [ 0] [   1/ 100] time: 72.1303, loss: 188.08206177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:244: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:245: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/home/johnyi/.local/lib/python3.5/site-packages/imageio/core/util.py:104: UserWarning: Conversion from float32 to uint8, range [0.0, 255.00006103515625]\n",
      "  'range [{2}, {3}]'.format(dtype_str, out_type.__name__, mi, ma))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSNR original: 21.325452844498134 MCT:  15.959287965164444\n",
      "Epoch: [ 0] test PSNR original, MTC: 21.325453, 15.959288\n",
      "Loading videos again...\n",
      "loaded video indexes: [43, 53, 73, 14, 27, 19, 31, 72, 63, 16, 1, 0, 49, 2, 15, 67, 64, 38, 68, 65, 56, 42, 34, 17, 48, 39, 24, 29, 45, 37] num_frames per video: 50 runtime:  74.37800860404968\n",
      "Epoch: [100] [   1/ 100] time: 223.0540, loss: 1359.01306152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/johnyi/.local/lib/python3.5/site-packages/imageio/core/util.py:104: UserWarning: Conversion from float32 to uint8, range [-32.0, 255.00003051757812]\n",
      "  'range [{2}, {3}]'.format(dtype_str, out_type.__name__, mi, ma))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSNR original: 21.325452844498134 MCT:  15.875836276693335\n",
      "Epoch: [100] test PSNR original, MTC: 21.325453, 15.875836\n",
      "Loading videos again...\n",
      "loaded video indexes: [66, 13, 36, 4, 23, 46, 54, 67, 53, 33, 44, 69, 14, 72, 64, 32, 51, 12, 39, 35, 18, 57, 19, 15, 38, 26, 59, 1, 60, 75] num_frames per video: 50 runtime:  73.23020839691162\n",
      "Epoch: [200] [   1/ 100] time: 372.4361, loss: 615.30114746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/johnyi/.local/lib/python3.5/site-packages/imageio/core/util.py:104: UserWarning: Conversion from float32 to uint8, range [0.0, 255.00009155273438]\n",
      "  'range [{2}, {3}]'.format(dtype_str, out_type.__name__, mi, ma))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSNR original: 21.325452844498134 MCT:  15.919345656392693\n",
      "Epoch: [200] test PSNR original, MTC: 21.325453, 15.919346\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-3cc859d21cb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mMCT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-0d53035b39d5>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, config, load)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 _, loss = self.sess.run([self.optimizer, self.loss],\n\u001b[0;32m--> 216\u001b[0;31m                     feed_dict={ self.input_t0: batch_t0, self.input_t1: batch_t1 })\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0mcounter\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with sess.as_default():\n",
    "    MCT.train(config, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [*] Reading checkpoints...\n",
      "loading from  checkpoint/MCT/CDVL\n",
      "INFO:tensorflow:Restoring parameters from checkpoint/MCT/CDVL/MCT-RGB\n",
      " [*] Load SUCCESS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:244: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:245: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSNR original: 21.325452844498134 MCT:  15.797822092797935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/johnyi/.local/lib/python3.5/site-packages/imageio/core/util.py:104: UserWarning: Conversion from float32 to uint8, range [0.0, 255.00009155273438]\n",
      "  'range [{2}, {3}]'.format(dtype_str, out_type.__name__, mi, ma))\n"
     ]
    }
   ],
   "source": [
    "with sess.as_default():\n",
    "    MCT.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sess.as_default():\n",
    "    start_time = time.time()\n",
    "    a,b = espcn.test(name = \"Set5\", load = True)\n",
    "    print(time.time()-start_time)\n",
    "    print(\"avg:\",a,\"bicubic:\",b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
